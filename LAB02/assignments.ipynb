{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Đã load dữ liệu: (4424, 37)\n",
      "\n",
      "Phân phối nhãn ban đầu:\n",
      "Target\n",
      "Graduate    2209\n",
      "Dropout     1421\n",
      "Enrolled     794\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tỷ lệ (%):\n",
      "Target\n",
      "Graduate    49.932188\n",
      "Dropout     32.120253\n",
      "Enrolled    17.947559\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', sep=';')\n",
    "\n",
    "print(f\"\\n✓ Đã load dữ liệu: {data.shape}\")\n",
    "print(f\"\\nPhân phối nhãn ban đầu:\")\n",
    "print(data['Target'].value_counts())\n",
    "print(f\"\\nTỷ lệ (%):\")\n",
    "print(data['Target'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xử lý nhãn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chỉ giữ lại Graduate và Dropout, loại bỏ Enrolled\n",
    "data = data[data['Target'].isin(['Graduate', 'Dropout'])].copy()\n",
    "\n",
    "# Chuyển đổi nhãn\n",
    "data['Target'] = data['Target'].map({'Graduate': 1, 'Dropout': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chuẩn hóa dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuẩn hóa các cột float\n",
    "normalize_columns = [\n",
    "    \"Previous qualification (grade)\",\n",
    "    \"Admission grade\",\n",
    "    \"Unemployment rate\",\n",
    "    \"Inflation rate\",\n",
    "    \"GDP\"\n",
    "]\n",
    "\n",
    "for column in normalize_columns:\n",
    "    data[column] = (data[column] - data[column].mean()) / data[column].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tách X và y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Shape của X (raw): (3630, 36)\n"
     ]
    }
   ],
   "source": [
    "X_raw = data.iloc[:, :-1].values\n",
    "\n",
    "print(f\"✓ Shape của X (raw): {X_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chuẩn hóa min - max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(X):\n",
    "    \"\"\"Chuẩn hóa dữ liệu về [0, 1]\"\"\"\n",
    "    X_min = X.min(axis=0)\n",
    "    X_max = X.max(axis=0)\n",
    "    return (X - X_min) / (X_max - X_min + 1e-8)\n",
    "\n",
    "X_scaled = min_max_scale(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thêm cột Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Shape của X (sau scaling + bias): (3630, 37)\n",
      "✓ X có NaN: False\n",
      "✓ X có Inf: False\n"
     ]
    }
   ],
   "source": [
    "# Thêm bias term\n",
    "X = np.hstack([np.ones((X_scaled.shape[0], 1)), X_scaled])\n",
    "\n",
    "print(f\"✓ Shape của X (sau scaling + bias): {X.shape}\")\n",
    "print(f\"✓ X có NaN: {np.isnan(X).any()}\")\n",
    "print(f\"✓ X có Inf: {np.isinf(X).any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1 (4 scores):\n",
    "\n",
    "- Use Numpy only to construct the Logistic Regression model.\n",
    "- Train that Logistic Regression model dataset using the Gradient Descend approach on the [Predict students’ dropout and academic success](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success) dataset. *Note that three class in this dataset must be merge into two class as: graduate and non-graduate (dropout or enroll)*.\n",
    "- Evaluate that Logistic Regression model on the [Predict students’ dropout and academic success](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success) dataset.\n",
    "- Visualize the loss function of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, epoch: int = 100, lr: float = 0.1, reg_lambda: float = 0.01):\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.theta = None\n",
    "\n",
    "    def _sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        z_clip = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z_clip))\n",
    "\n",
    "    def _compute_loss(self, y: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "        eps = 1e-15\n",
    "        y_hat = np.clip(y_hat, eps, 1 - eps)\n",
    "        n = len(y)\n",
    "        loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "        if self.reg_lambda > 0:\n",
    "            reg_term = (self.reg_lambda / (2 * n)) * np.sum(self.theta[1:] ** 2)\n",
    "            loss += reg_term\n",
    "        return loss\n",
    "\n",
    "    def _compute_accuracy(self, y: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "        predictions = (y_hat >= 0.5).astype(int)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.random.randn(n_features, 1) * np.sqrt(2.0 / n_features)\n",
    "        \n",
    "        with tqdm(range(self.epoch), desc=\"Logistic Regression\") as pb:\n",
    "            for e in pb:\n",
    "                # Forward\n",
    "                z = X @ self.theta\n",
    "                y_hat = self._sigmoid(z)\n",
    "                \n",
    "                # Backward\n",
    "                grad = (1 / n_samples) * (X.T @ (y_hat - y))\n",
    "                if self.reg_lambda > 0:\n",
    "                    reg_grad = np.vstack([\n",
    "                        np.zeros((1, 1)),\n",
    "                        (self.reg_lambda / n_samples) * self.theta[1:]\n",
    "                    ])\n",
    "                    grad += reg_grad\n",
    "                \n",
    "                # Gradient clipping\n",
    "                grad_norm = np.linalg.norm(grad)\n",
    "                if grad_norm > 5:\n",
    "                    grad = 5 * grad / grad_norm\n",
    "                \n",
    "                # Update\n",
    "                self.theta -= self.lr * grad\n",
    "                \n",
    "                # Metrics\n",
    "                loss = self._compute_loss(y, y_hat)\n",
    "                acc = self._compute_accuracy(y, y_hat)\n",
    "                \n",
    "                pb.set_postfix({\"loss\": f\"{loss:.4f}\", \"acc\": f\"{acc:.4f}\"})\n",
    "                \n",
    "                self.losses.append(loss)\n",
    "                self.accuracies.append(acc)\n",
    "                \n",
    "                if e > 10 and abs(self.losses[-1] - self.losses[-2]) < 1e-6:\n",
    "                    break\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        z = X @ self.theta\n",
    "        return self._sigmoid(z)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (0, 37)\n",
      "y shape: (0, 1)\n",
      "X dtype: float64\n",
      "y dtype: int64\n",
      "X_arr shape: (0, 37)\n",
      "y_arr shape: (0, 1)\n",
      ">>> Tập dữ liệu rỗng: X không có mẫu.\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", getattr(X_logistic, \"shape\", None))\n",
    "print(\"y shape:\", getattr(y_logistic, \"shape\", None))\n",
    "print(\"X dtype:\", getattr(X_logistic, \"dtype\", None))\n",
    "print(\"y dtype:\", getattr(y_logistic, \"dtype\", None))\n",
    "\n",
    "# Nếu là pandas object, xem số hàng sau khi chuyển sang numpy\n",
    "import numpy as np\n",
    "X_arr = np.asarray(X_logistic)\n",
    "y_arr = np.asarray(y_logistic).reshape(-1, 1) if np.asarray(y_logistic).ndim == 1 else np.asarray(y_logistic)\n",
    "\n",
    "print(\"X_arr shape:\", X_arr.shape)\n",
    "print(\"y_arr shape:\", y_arr.shape)\n",
    "if X_arr.shape[0] == 0:\n",
    "    print(\">>> Tập dữ liệu rỗng: X không có mẫu.\")\n",
    "if X_arr.shape[0] != y_arr.shape[0]:\n",
    "    print(\">>> Số hàng X và y KHÔNG KHỚP.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Logistic Regression ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logistic Regression:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training Logistic Regression ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m logistic_model \u001b[38;5;241m=\u001b[39m LogisticRegression(epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, reg_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m logistic_model\u001b[38;5;241m.\u001b[39mfit(X_logistic, y_logistic)\n",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     36\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sigmoid(z)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m grad \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m n_samples) \u001b[38;5;241m*\u001b[39m (X\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m (y_hat \u001b[38;5;241m-\u001b[39m y))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_lambda \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     41\u001b[0m     reg_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([\n\u001b[1;32m     42\u001b[0m         np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     43\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_lambda \u001b[38;5;241m/\u001b[39m n_samples) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     44\u001b[0m     ])\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training Logistic Regression ---\")\n",
    "logistic_model = LogisticRegression(epoch=100, lr=0.1, reg_lambda=0.01)\n",
    "logistic_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KẾT QUẢ CUỐI CÙNG ===\n",
      "Accuracy trên toàn bộ dữ liệu: 0.7931\n",
      "Loss cuối cùng: 0.4996\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 719  702]\n",
      " [  49 2160]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.94      0.51      0.66      1421\n",
      "    Graduate       0.75      0.98      0.85      2209\n",
      "\n",
      "    accuracy                           0.79      3630\n",
      "   macro avg       0.85      0.74      0.75      3630\n",
      "weighted avg       0.83      0.79      0.78      3630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "final_acc = np.mean(y_pred == y)\n",
    "print(f\"\\n=== KẾT QUẢ CUỐI CÙNG ===\")\n",
    "print(f\"Accuracy trên toàn bộ dữ liệu: {final_acc:.4f}\")\n",
    "print(f\"Loss cuối cùng: {model.losses[-1]:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y, y_pred, target_names=['Dropout', 'Graduate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2 (4 scores):\n",
    "\n",
    "- Use Numpy only to construct the Sofmax Regression model.\n",
    "- Train that Logistic Regression model dataset using the Gradient Descend approach on the [Predict students’ dropout and academic success](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success) dataset.\n",
    "- Evaluate that Logistic Regression model on the [Predict students’ dropout and academic success](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success) dataset.\n",
    "- Visualize the loss function of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Softmax Regression (Multi-class Classification) ---\n",
      "✓ Số samples: 3630\n",
      "✓ Graduate: 0\n",
      "✓ Dropout: 0\n",
      "✓ Enrolled: 0\n",
      "✓ X_softmax shape: (3630, 37)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_one_hot\n\u001b[1;32m     20\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m---> 21\u001b[0m y_softmax \u001b[38;5;241m=\u001b[39m one_hot_encode(y_softmax_labels, num_classes)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ y_softmax shape (one-hot): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_softmax\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36mone_hot_encode\u001b[0;34m(y, num_classes)\u001b[0m\n\u001b[1;32m     15\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m     16\u001b[0m y_one_hot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n, num_classes))\n\u001b[0;32m---> 17\u001b[0m y_one_hot[np\u001b[38;5;241m.\u001b[39marange(n), y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_one_hot\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Nhãn cho Softmax Regression (Multi-class: 3 classes)\n",
    "print(\"\\n--- Softmax Regression (Multi-class Classification) ---\")\n",
    "label_mapping = {'Graduate': 0, 'Dropout': 1, 'Enrolled': 2}\n",
    "y_softmax_labels = data['Target'].map(label_mapping).values\n",
    "X_softmax = X.copy()\n",
    "\n",
    "print(f\"✓ Số samples: {len(y_softmax_labels)}\")\n",
    "for label, idx in label_mapping.items():\n",
    "    count = np.sum(y_softmax_labels == idx)\n",
    "    print(f\"✓ {label}: {count}\")\n",
    "print(f\"✓ X_softmax shape: {X_softmax.shape}\")\n",
    "\n",
    "# One-hot encoding\n",
    "def one_hot_encode(y, num_classes):\n",
    "    n = len(y)\n",
    "    y_one_hot = np.zeros((n, num_classes))\n",
    "    y_one_hot[np.arange(n), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "num_classes = 3\n",
    "y_softmax = one_hot_encode(y_softmax_labels, num_classes)\n",
    "print(f\"✓ y_softmax shape (one-hot): {y_softmax.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self, epoch: int = 200, lr: float = 0.1, reg_lambda: float = 0.01):\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.theta = None\n",
    "        \n",
    "    def _softmax(self, z: np.ndarray) -> np.ndarray:\n",
    "        z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z_shifted)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def _compute_loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        n = len(y_true)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / n\n",
    "        if self.reg_lambda > 0:\n",
    "            reg_term = (self.reg_lambda / (2 * n)) * np.sum(self.theta[1:, :] ** 2)\n",
    "            loss += reg_term\n",
    "        return loss\n",
    "    \n",
    "    def _compute_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        return np.mean(y_true_labels == y_pred_labels)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        n_samples, n_features = X.shape\n",
    "        num_classes = y.shape[1]\n",
    "        self.theta = np.random.randn(n_features, num_classes) * 0.01\n",
    "        \n",
    "        with tqdm(range(self.epoch), desc=\"Softmax Regression\") as pb:\n",
    "            for e in pb:\n",
    "                # Forward\n",
    "                z = X @ self.theta\n",
    "                y_pred = self._softmax(z)\n",
    "                \n",
    "                # Backward\n",
    "                grad = (1 / n_samples) * (X.T @ (y_pred - y))\n",
    "                if self.reg_lambda > 0:\n",
    "                    reg_grad = np.vstack([\n",
    "                        np.zeros((1, num_classes)),\n",
    "                        (self.reg_lambda / n_samples) * self.theta[1:, :]\n",
    "                    ])\n",
    "                    grad += reg_grad\n",
    "                \n",
    "                # Gradient clipping\n",
    "                grad_norm = np.linalg.norm(grad)\n",
    "                if grad_norm > 5:\n",
    "                    grad = 5 * grad / grad_norm\n",
    "                \n",
    "                # Update\n",
    "                self.theta -= self.lr * grad\n",
    "                \n",
    "                # Metrics\n",
    "                loss = self._compute_loss(y, y_pred)\n",
    "                acc = self._compute_accuracy(y, y_pred)\n",
    "                \n",
    "                pb.set_postfix({\"loss\": f\"{loss:.4f}\", \"acc\": f\"{acc:.4f}\"})\n",
    "                \n",
    "                self.losses.append(loss)\n",
    "                self.accuracies.append(acc)\n",
    "                \n",
    "                if e > 10 and abs(self.losses[-1] - self.losses[-2]) < 1e-6:\n",
    "                    break\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        z = X @ self.theta\n",
    "        return self._softmax(z)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "\n",
    "print(\"✓ Định nghĩa SoftmaxRegression class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Training Softmax Regression ---\")\n",
    "softmax_model = SoftmaxRegression(epoch=200, lr=0.1, reg_lambda=0.01)\n",
    "softmax_model.fit(X_softmax, y_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"SOFTMAX REGRESSION (Multi-class Classification)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "y_pred_softmax = softmax_model.predict(X_softmax)\n",
    "y_true_softmax = y_softmax_labels\n",
    "\n",
    "acc_softmax = np.mean(y_pred_softmax == y_true_softmax)\n",
    "print(f\"\\nAccuracy: {acc_softmax:.4f} ({acc_softmax*100:.2f}%)\")\n",
    "print(f\"Final Loss: {softmax_model.losses[-1]:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_softmax = confusion_matrix(y_true_softmax, y_pred_softmax)\n",
    "print(cm_softmax)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_names = ['Graduate', 'Dropout', 'Enrolled']\n",
    "print(classification_report(y_true_softmax, y_pred_softmax, \n",
    "                          target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3 (2 scores):\n",
    "\n",
    "- Use a Machine Learning library (Scikit Learn or Skorch) to implement and evaluate the Logistic Regression on the [Predict students’ dropout and academic success](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success) dataset.\n",
    "- Use a Machine Learning library (Scikit Learn or Skorch) to implement and evaluate the Softmax Regression on the [Predict students’ dropout and academic success](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success) dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
